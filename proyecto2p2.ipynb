{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5cec2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos y convertir a numéricos\n",
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "\n",
    "# Eliminar columnas no numéricas y convertir\n",
    "X_train = train_df.drop(columns=[\"paciente_id\", \"target\"]).apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "y_train = train_df.loc[X_train.index, \"target\"].values.reshape(-1, 1)\n",
    "X_test = test_df.drop(columns=[\"paciente_id\"]).apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "# Normalizar\n",
    "X_train_normalized = normalize(X_train.values.astype(np.float64))\n",
    "X_test_normalized = normalize(X_test.values.astype(np.float64))\n",
    "\n",
    "# Añadir bias\n",
    "X_train_final = np.hstack([np.ones((X_train_normalized.shape[0], 1)), X_train_normalized])\n",
    "X_test_final = np.hstack([np.ones((X_test_normalized.shape[0], 1)), X_test_normalized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfe9244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def train_logistic_regression(X, y, lr=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    bias = 0\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        z = np.dot(X, theta) + bias\n",
    "        y_hat = sigmoid(z)\n",
    "\n",
    "        loss = compute_loss(y, y_hat)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        dz = y_hat - y\n",
    "        dw = np.dot(X.T, dz) / m\n",
    "        db = np.sum(dz) / m\n",
    "\n",
    "        theta -= lr * dw\n",
    "        bias -= lr * db\n",
    "\n",
    "    return theta, bias, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7bea8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta, bias, threshold=0.5):\n",
    "    probs = sigmoid(np.dot(X, theta) + bias)\n",
    "    return (probs >= threshold).astype(int)\n",
    "\n",
    "def compute_f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    if tp + fp == 0 or tp + fn == 0:\n",
    "        return 0.0  # evitar división por cero\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def normalize(X):\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    # Evitar división por cero\n",
    "    norms[norms == 0] = 1\n",
    "    return X / norms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1776a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar tu dataframe (asegúrate de que esté en tu entorno)\n",
    "train_df = pd.read_csv(\"train_df.csv\")  # Reemplaza con la ruta a tu archivo CSV\n",
    "df = train_df.copy()\n",
    "df = df.drop(columns=[\"paciente_id\"])         # 1. Eliminar ID\n",
    "df[\"genero\"] = df[\"genero\"].map({\"M\": 1, \"F\": 0})  # 2. Codificar genero\n",
    "\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"].values.reshape(-1, 1)\n",
    "\n",
    "# 3. Estandarizar variables numéricas (menos 'genero')\n",
    "numeric_cols = X.columns.tolist()\n",
    "numeric_cols.remove(\"genero\")\n",
    "\n",
    "means = X[numeric_cols].mean()\n",
    "stds = X[numeric_cols].std()\n",
    "X[numeric_cols] = (X[numeric_cols] - means) / stds\n",
    "\n",
    "X_final = X.values  # X_final vuelve a existir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dadcae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X, y, val_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[0]\n",
    "    indices = np.random.permutation(m)\n",
    "    val_size = int(m * val_ratio)\n",
    "    \n",
    "    val_idx = indices[:val_size]\n",
    "    train_idx = indices[val_size:]\n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce4cd076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score en Validación: 0.4576271186440678\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = train_val_split(X_final, y)\n",
    "theta, bias, losses = train_logistic_regression(X_train, y_train, lr=0.1, epochs=1000)\n",
    "y_val_pred = predict(X_val, theta, bias)\n",
    "f1_val = compute_f1_score(y_val.ravel(), y_val_pred.ravel())\n",
    "\n",
    "print(\"F1-Score en Validación:\", f1_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20d0dd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando combinaciones...\n",
      "\n",
      "lr = 0.01, epochs = 500 => F1-score: 0.4636\n",
      "lr = 0.01, epochs = 1000 => F1-score: 0.4584\n",
      "lr = 0.01, epochs = 2000 => F1-score: 0.4571\n",
      "lr = 0.05, epochs = 500 => F1-score: 0.4595\n",
      "lr = 0.05, epochs = 1000 => F1-score: 0.4552\n",
      "lr = 0.05, epochs = 2000 => F1-score: 0.4576\n",
      "lr = 0.1, epochs = 500 => F1-score: 0.4552\n",
      "lr = 0.1, epochs = 1000 => F1-score: 0.4576\n",
      "lr = 0.1, epochs = 2000 => F1-score: 0.4610\n",
      "lr = 0.2, epochs = 500 => F1-score: 0.4576\n",
      "lr = 0.2, epochs = 1000 => F1-score: 0.4610\n",
      "lr = 0.2, epochs = 2000 => F1-score: 0.4615\n",
      "\n",
      "✅ Mejor combinación:\n",
      "Learning Rate: 0.01\n",
      "Epochs: 500\n",
      "F1-Score: 0.4636\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
    "epoch_list = [500, 1000, 2000]\n",
    "\n",
    "best_f1 = 0\n",
    "best_params = (None, None)\n",
    "\n",
    "print(\"Probando combinaciones...\\n\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in epoch_list:\n",
    "        theta, bias, _ = train_logistic_regression(X_train, y_train, lr=lr, epochs=epochs)\n",
    "        y_val_pred = predict(X_val, theta, bias)\n",
    "        f1 = compute_f1_score(y_val.ravel(), y_val_pred.ravel())\n",
    "        \n",
    "        print(f\"lr = {lr}, epochs = {epochs} => F1-score: {f1:.4f}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_params = (lr, epochs)\n",
    "\n",
    "print(\"\\n✅ Mejor combinación:\")\n",
    "print(f\"Learning Rate: {best_params[0]}\")\n",
    "print(f\"Epochs: {best_params[1]}\")\n",
    "print(f\"F1-Score: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c8b3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_final, bias_final, _ = train_logistic_regression(X_final, y, lr=0.01, epochs=500)\n",
    "\n",
    "test_df = pd.read_csv(r\"C:\\Users\\dfqo2\\Desktop\\ALC\\Kaggle\\Proyecto2\\test_df.csv\")\n",
    "test_df = test_df.drop(columns=[\"paciente_id\"])  # Eliminar ID\n",
    "test_df[\"genero\"] = test_df[\"genero\"].map({\"M\": 1, \"F\": 0})  # Codificar genero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d7c756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_final shape: (10500, 15)\n",
      "theta shape: (14, 1)\n"
     ]
    }
   ],
   "source": [
    "# Excluir columnas no numéricas o irrelevantes\n",
    "train_columns = [col for col in train_df.columns if col not in ['paciente_id', 'target']]\n",
    "\n",
    "# Dejar solo esas columnas en el test\n",
    "X_test = test_df[train_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Rellenar valores faltantes\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "# Normalizar\n",
    "X_test_normalized = normalize(X_test.values.astype(np.float64))\n",
    "\n",
    "# Añadir bias\n",
    "X_test_final = np.hstack([np.ones((X_test_normalized.shape[0], 1)), X_test_normalized])\n",
    "\n",
    "# Confirmar dimensiones\n",
    "print(\"X_test_final shape:\", X_test_final.shape)\n",
    "print(\"theta shape:\", theta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24775c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo submission.csv guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos\n",
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "\n",
    "# Preprocesamiento\n",
    "df = train_df.copy()\n",
    "df[\"genero\"] = df[\"genero\"].map({\"M\": 1, \"F\": 0})  # Codificar genero\n",
    "paciente_ids = df[\"paciente_id\"].values  # Guardar IDs\n",
    "df = df.drop(columns=[\"paciente_id\", \"target\"])\n",
    "\n",
    "# Normalización\n",
    "def normalize(X):\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1\n",
    "    return X / norms\n",
    "\n",
    "# Estándarización numérica (excepto 'genero')\n",
    "numeric_cols = df.columns.tolist()\n",
    "numeric_cols.remove(\"genero\")\n",
    "means = df[numeric_cols].mean()\n",
    "stds = df[numeric_cols].std()\n",
    "df[numeric_cols] = (df[numeric_cols] - means) / stds\n",
    "\n",
    "# Convertir a matriz\n",
    "X = df.values.astype(np.float64)\n",
    "X_normalized = normalize(X)\n",
    "\n",
    "# NO añadimos bias manualmente, el modelo ya lo tiene separado\n",
    "X_with_bias = X_normalized\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_probs = sigmoid(np.dot(X_with_bias, theta_final) + bias_final)\n",
    "y_pred_labels = (y_pred_probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "# Crear archivo de submission\n",
    "submission_df = pd.DataFrame({\n",
    "    \"paciente_id\": paciente_ids,\n",
    "    \"target\": y_pred_labels\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ Archivo submission.csv guardado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fcc4ab07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mejor threshold: 0.10 con F1-score: 0.6677\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "\n",
    "y_val_probs = sigmoid(np.dot(X_val, theta) + bias)\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_val_probs >= t).astype(int)\n",
    "    f1 = compute_f1_score(y_val.ravel(), y_pred_thresh.ravel())\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"✅ Mejor threshold: {best_thresh:.2f} con F1-score: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dcc922b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_polynomial_features(X_array):\n",
    "    # Si no tenemos nombres de columnas, los inventamos: col_0, col_1, ...\n",
    "    num_features = X_array.shape[1]\n",
    "    column_names = [f\"col_{i}\" for i in range(num_features)]\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    X_df = pd.DataFrame(X_array, columns=column_names)\n",
    "\n",
    "    # Asumimos que no hay una columna \"genero\" ya codificada. Si la hay, podemos adaptarlo.\n",
    "    numeric_cols = X_df.columns.tolist()\n",
    "\n",
    "    # Crear nuevas características\n",
    "    for col in numeric_cols:\n",
    "        X_df[f\"{col}_squared\"] = X_df[col] ** 2\n",
    "\n",
    "    for i in range(len(numeric_cols)):\n",
    "        for j in range(i+1, len(numeric_cols)):\n",
    "            col1 = numeric_cols[i]\n",
    "            col2 = numeric_cols[j]\n",
    "            X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
    "\n",
    "    return X_df\n",
    "\n",
    "# Aplicar\n",
    "X_poly_df = add_polynomial_features(X)\n",
    "\n",
    "# Normalizar (todas las columnas)\n",
    "columns_to_scale = X_poly_df.columns.tolist()\n",
    "means = X_poly_df[columns_to_scale].mean()\n",
    "stds = X_poly_df[columns_to_scale].std()\n",
    "X_poly_df[columns_to_scale] = (X_poly_df[columns_to_scale] - means) / stds\n",
    "\n",
    "# Convertir de nuevo a numpy para entrenamiento\n",
    "X_final_poly = X_poly_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a42ae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mejor threshold con features extendidas: 0.25, F1: 0.6697\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = train_val_split(X_final_poly, y)\n",
    "theta, bias, _ = train_logistic_regression(X_train, y_train, lr=0.1, epochs=1000)\n",
    "\n",
    "# Probar de nuevo thresholds\n",
    "y_val_probs = sigmoid(np.dot(X_val, theta) + bias)\n",
    "thresholds = np.arange(0.05, 0.9, 0.05)\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_val_probs >= t).astype(int)\n",
    "    f1 = compute_f1_score(y_val.ravel(), y_pred_thresh.ravel())\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"✅ Mejor threshold con features extendidas: {best_thresh:.2f}, F1: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38db1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_correlation(X, y):\n",
    "    \"\"\"\n",
    "    Calcula la correlación de Pearson entre cada columna de X y el vector y.\n",
    "    \"\"\"\n",
    "    mean_y = np.mean(y)\n",
    "    correlations = []\n",
    "    for i in range(X.shape[1]):\n",
    "        xi = X[:, i]\n",
    "        mean_xi = np.mean(xi)\n",
    "        numerator = np.sum((xi - mean_xi) * (y - mean_y))\n",
    "        denominator = np.sqrt(np.sum((xi - mean_xi)**2) * np.sum((y - mean_y)**2))\n",
    "        corr = numerator / (denominator + 1e-8)\n",
    "        correlations.append(corr)\n",
    "    return np.array(correlations)\n",
    "\n",
    "def select_top_features(X, y, top_k=10):\n",
    "    corrs = feature_correlation(X, y)\n",
    "    top_indices = np.argsort(np.abs(corrs))[-top_k:]\n",
    "    return X[:, top_indices], top_indices  # retorna también los índices para seguimiento\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70c20536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
      "C:\\Users\\dfqo2\\AppData\\Local\\Temp\\ipykernel_29384\\4171461941.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ submission.csv generado con threshold = 0.25 usando train_df.\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "\n",
    "# --- Preprocesamiento de train_df ---\n",
    "train_df[\"genero\"] = train_df[\"genero\"].map({\"M\": 1, \"F\": 0})\n",
    "y = train_df[\"target\"].values.reshape(-1, 1)\n",
    "paciente_ids_train = train_df[\"paciente_id\"].values\n",
    "X_df = train_df.drop(columns=[\"paciente_id\", \"target\"])\n",
    "\n",
    "# Normalizar columnas numéricas (menos 'genero')\n",
    "numeric_cols = X_df.columns.tolist()\n",
    "numeric_cols.remove(\"genero\")\n",
    "means = X_df[numeric_cols].mean()\n",
    "stds = X_df[numeric_cols].std()\n",
    "X_df[numeric_cols] = (X_df[numeric_cols] - means) / stds\n",
    "\n",
    "# Convertir a numpy y normalizar L2\n",
    "X = X_df.values.astype(np.float64)\n",
    "X = normalize(X)\n",
    "\n",
    "# Features polinomiales\n",
    "X_poly_df = add_polynomial_features(X)\n",
    "\n",
    "# Normalizar features extendidas\n",
    "means_poly = X_poly_df.mean()\n",
    "stds_poly = X_poly_df.std()\n",
    "X_poly_df = (X_poly_df - means_poly) / stds_poly\n",
    "X_final_poly = X_poly_df.values\n",
    "\n",
    "# Entrenar el modelo final\n",
    "theta, bias = train_logistic_regression(X_final_poly, y, lr=0.1, epochs=1000)\n",
    "\n",
    "# Generar predicciones para los mismos datos de entrenamiento\n",
    "y_probs = sigmoid(np.dot(X_final_poly, theta) + bias)\n",
    "\n",
    "# Usar el mejor threshold encontrado\n",
    "best_thresh = 0.25\n",
    "y_preds = (y_probs >= best_thresh).astype(int).flatten()\n",
    "\n",
    "# Crear archivo de submission con train\n",
    "submission_df = pd.DataFrame({\n",
    "    \"paciente_id\": paciente_ids_train,\n",
    "    \"target\": y_preds\n",
    "})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ submission.csv generado con threshold = 0.25 usando train_df.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
