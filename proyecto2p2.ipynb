{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos y convertir a num√©ricos\n",
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "\n",
    "# Eliminar columnas no num√©ricas y convertir\n",
    "X_train = train_df.drop(columns=[\"paciente_id\", \"target\"]).apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "y_train = train_df.loc[X_train.index, \"target\"].values.reshape(-1, 1)\n",
    "X_test = test_df.drop(columns=[\"paciente_id\"]).apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "# Normalizar\n",
    "X_train_normalized = normalize(X_train.values.astype(np.float64))\n",
    "X_test_normalized = normalize(X_test.values.astype(np.float64))\n",
    "\n",
    "# A√±adir bias\n",
    "X_train_final = np.hstack([np.ones((X_train_normalized.shape[0], 1)), X_train_normalized])\n",
    "X_test_final = np.hstack([np.ones((X_test_normalized.shape[0], 1)), X_test_normalized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def train_logistic_regression(X, y, lr=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    bias = 0\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        z = np.dot(X, theta) + bias\n",
    "        y_hat = sigmoid(z)\n",
    "\n",
    "        loss = compute_loss(y, y_hat)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        dz = y_hat - y\n",
    "        dw = np.dot(X.T, dz) / m\n",
    "        db = np.sum(dz) / m\n",
    "\n",
    "        theta -= lr * dw\n",
    "        bias -= lr * db\n",
    "\n",
    "    return theta, bias, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bea8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta, bias, threshold=0.5):\n",
    "    probs = sigmoid(np.dot(X, theta) + bias)\n",
    "    return (probs >= threshold).astype(int)\n",
    "\n",
    "def compute_f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    if tp + fp == 0 or tp + fn == 0:\n",
    "        return 0.0  # evitar divisi√≥n por cero\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def normalize(X):\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    # Evitar divisi√≥n por cero\n",
    "    norms[norms == 0] = 1\n",
    "    return X / norms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar tu dataframe (aseg√∫rate de que est√© en tu entorno)\n",
    "train_df = pd.read_csv(\"train_df.csv\")  # Reemplaza con la ruta a tu archivo CSV\n",
    "df = train_df.copy()\n",
    "df = df.drop(columns=[\"paciente_id\"])         # 1. Eliminar ID\n",
    "df[\"genero\"] = df[\"genero\"].map({\"M\": 1, \"F\": 0})  # 2. Codificar genero\n",
    "\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"].values.reshape(-1, 1)\n",
    "\n",
    "# 3. Estandarizar variables num√©ricas (menos 'genero')\n",
    "numeric_cols = X.columns.tolist()\n",
    "numeric_cols.remove(\"genero\")\n",
    "\n",
    "means = X[numeric_cols].mean()\n",
    "stds = X[numeric_cols].std()\n",
    "X[numeric_cols] = (X[numeric_cols] - means) / stds\n",
    "\n",
    "X_final = X.values  # X_final vuelve a existir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X, y, val_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[0]\n",
    "    indices = np.random.permutation(m)\n",
    "    val_size = int(m * val_ratio)\n",
    "    \n",
    "    val_idx = indices[:val_size]\n",
    "    train_idx = indices[val_size:]\n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4cd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = train_val_split(X_final, y)\n",
    "theta, bias, losses = train_logistic_regression(X_train, y_train, lr=0.1, epochs=1000)\n",
    "y_val_pred = predict(X_val, theta, bias)\n",
    "f1_val = compute_f1_score(y_val.ravel(), y_val_pred.ravel())\n",
    "\n",
    "print(\"F1-Score en Validaci√≥n:\", f1_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
    "epoch_list = [500, 1000, 2000]\n",
    "\n",
    "best_f1 = 0\n",
    "best_params = (None, None)\n",
    "\n",
    "print(\"Probando combinaciones...\\n\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in epoch_list:\n",
    "        theta, bias, _ = train_logistic_regression(X_train, y_train, lr=lr, epochs=epochs)\n",
    "        y_val_pred = predict(X_val, theta, bias)\n",
    "        f1 = compute_f1_score(y_val.ravel(), y_val_pred.ravel())\n",
    "        \n",
    "        print(f\"lr = {lr}, epochs = {epochs} => F1-score: {f1:.4f}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_params = (lr, epochs)\n",
    "\n",
    "print(\"\\n‚úÖ Mejor combinaci√≥n:\")\n",
    "print(f\"Learning Rate: {best_params[0]}\")\n",
    "print(f\"Epochs: {best_params[1]}\")\n",
    "print(f\"F1-Score: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_final, bias_final, _ = train_logistic_regression(X_final, y, lr=0.01, epochs=500)\n",
    "\n",
    "test_df = pd.read_csv(r\"C:\\Users\\dfqo2\\Desktop\\ALC\\Kaggle\\Proyecto2\\test_df.csv\")\n",
    "test_df = test_df.drop(columns=[\"paciente_id\"])  # Eliminar ID\n",
    "test_df[\"genero\"] = test_df[\"genero\"].map({\"M\": 1, \"F\": 0})  # Codificar genero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir columnas no num√©ricas o irrelevantes\n",
    "train_columns = [col for col in train_df.columns if col not in ['paciente_id', 'target']]\n",
    "\n",
    "# Dejar solo esas columnas en el test\n",
    "X_test = test_df[train_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Rellenar valores faltantes\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "# Normalizar\n",
    "X_test_normalized = normalize(X_test.values.astype(np.float64))\n",
    "\n",
    "# A√±adir bias\n",
    "X_test_final = np.hstack([np.ones((X_test_normalized.shape[0], 1)), X_test_normalized])\n",
    "\n",
    "# Confirmar dimensiones\n",
    "print(\"X_test_final shape:\", X_test_final.shape)\n",
    "print(\"theta shape:\", theta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24775c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos\n",
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "\n",
    "# Preprocesamiento\n",
    "df = train_df.copy()\n",
    "df[\"genero\"] = df[\"genero\"].map({\"M\": 1, \"F\": 0})  # Codificar genero\n",
    "paciente_ids = df[\"paciente_id\"].values  # Guardar IDs\n",
    "df = df.drop(columns=[\"paciente_id\", \"target\"])\n",
    "\n",
    "# Normalizaci√≥n\n",
    "def normalize(X):\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1\n",
    "    return X / norms\n",
    "\n",
    "# Est√°ndarizaci√≥n num√©rica (excepto 'genero')\n",
    "numeric_cols = df.columns.tolist()\n",
    "numeric_cols.remove(\"genero\")\n",
    "means = df[numeric_cols].mean()\n",
    "stds = df[numeric_cols].std()\n",
    "df[numeric_cols] = (df[numeric_cols] - means) / stds\n",
    "\n",
    "# Convertir a matriz\n",
    "X = df.values.astype(np.float64)\n",
    "X_normalized = normalize(X)\n",
    "\n",
    "# NO a√±adimos bias manualmente, el modelo ya lo tiene separado\n",
    "X_with_bias = X_normalized\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_probs = sigmoid(np.dot(X_with_bias, theta_final) + bias_final)\n",
    "y_pred_labels = (y_pred_probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "# Crear archivo de submission\n",
    "submission_df = pd.DataFrame({\n",
    "    \"paciente_id\": paciente_ids,\n",
    "    \"target\": y_pred_labels\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"‚úÖ Archivo submission.csv guardado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "\n",
    "y_val_probs = sigmoid(np.dot(X_val, theta) + bias)\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_val_probs >= t).astype(int)\n",
    "    f1 = compute_f1_score(y_val.ravel(), y_pred_thresh.ravel())\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"‚úÖ Mejor threshold: {best_thresh:.2f} con F1-score: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc922b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_polynomial_features(X_array):\n",
    "    # Si no tenemos nombres de columnas, los inventamos: col_0, col_1, ...\n",
    "    num_features = X_array.shape[1]\n",
    "    column_names = [f\"col_{i}\" for i in range(num_features)]\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    X_df = pd.DataFrame(X_array, columns=column_names)\n",
    "\n",
    "    # Asumimos que no hay una columna \"genero\" ya codificada. Si la hay, podemos adaptarlo.\n",
    "    numeric_cols = X_df.columns.tolist()\n",
    "\n",
    "    # Crear nuevas caracter√≠sticas\n",
    "    for col in numeric_cols:\n",
    "        X_df[f\"{col}_squared\"] = X_df[col] ** 2\n",
    "\n",
    "    for i in range(len(numeric_cols)):\n",
    "        for j in range(i+1, len(numeric_cols)):\n",
    "            col1 = numeric_cols[i]\n",
    "            col2 = numeric_cols[j]\n",
    "            X_df[f\"{col1}_x_{col2}\"] = X_df[col1] * X_df[col2]\n",
    "\n",
    "    return X_df\n",
    "\n",
    "# Aplicar\n",
    "X_poly_df = add_polynomial_features(X)\n",
    "\n",
    "# Normalizar (todas las columnas)\n",
    "columns_to_scale = X_poly_df.columns.tolist()\n",
    "means = X_poly_df[columns_to_scale].mean()\n",
    "stds = X_poly_df[columns_to_scale].std()\n",
    "X_poly_df[columns_to_scale] = (X_poly_df[columns_to_scale] - means) / stds\n",
    "\n",
    "# Convertir de nuevo a numpy para entrenamiento\n",
    "X_final_poly = X_poly_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a42ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = train_val_split(X_final_poly, y)\n",
    "\n",
    "theta, bias, _ = train_logistic_regression(X_train, y_train, lr=0.1, epochs=1000)\n",
    "\n",
    "# Probar de nuevo thresholds\n",
    "y_val_probs = sigmoid(np.dot(X_val, theta) + bias)\n",
    "thresholds = np.arange(0.000001, 1, 0.0000001)   \n",
    "\n",
    "best_f1 = 0\n",
    "best_thresh = 0.0001\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_val_probs >= t).astype(int)\n",
    "    f1 = compute_f1_score(y_val.ravel(), y_pred_thresh.ravel())\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"‚úÖ Mejor threshold con features extendidas: {best_thresh:.5f}, F1: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ Mejor threshold con features extendidas: {best_thresh:.10f}, F1: {best_f1:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4238a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f1_scores = []\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_val_probs >= t).astype(int)\n",
    "    f1 = compute_f1_score(y_val.ravel(), y_pred_thresh.ravel())\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Graficar el F1-score contra el threshold\n",
    "plt.plot(thresholds, f1_scores, marker='o')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('F1-score vs. Threshold')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_correlation(X, y):\n",
    "    \"\"\"\n",
    "    Calcula la correlaci√≥n de Pearson entre cada columna de X y el vector y.\n",
    "    \"\"\"\n",
    "    mean_y = np.mean(y)\n",
    "    correlations = []\n",
    "    for i in range(X.shape[1]):\n",
    "        xi = X[:, i]\n",
    "        mean_xi = np.mean(xi)\n",
    "        numerator = np.sum((xi - mean_xi) * (y - mean_y))\n",
    "        denominator = np.sqrt(np.sum((xi - mean_xi)**2) * np.sum((y - mean_y)**2))\n",
    "        corr = numerator / (denominator + 1e-8)\n",
    "        correlations.append(corr)\n",
    "    return np.array(correlations)\n",
    "\n",
    "def select_top_features(X, y, top_k=10):\n",
    "    corrs = feature_correlation(X, y)\n",
    "    top_indices = np.argsort(np.abs(corrs))[-top_k:]\n",
    "    return X[:, top_indices], top_indices  # retorna tambi√©n los √≠ndices para seguimiento\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c20536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "\n",
    "# --- Preprocesamiento de train_df ---\n",
    "train_df[\"genero\"] = train_df[\"genero\"].map({\"M\": 1, \"F\": 0})\n",
    "y = train_df[\"target\"].values.reshape(-1, 1)\n",
    "paciente_ids_train = train_df[\"paciente_id\"].values\n",
    "X_df = train_df.drop(columns=[\"paciente_id\", \"target\"])\n",
    "\n",
    "# Normalizar columnas num√©ricas (menos 'genero')\n",
    "numeric_cols = X_df.columns.tolist()\n",
    "numeric_cols.remove(\"genero\")\n",
    "means = X_df[numeric_cols].mean()\n",
    "stds = X_df[numeric_cols].std()\n",
    "X_df[numeric_cols] = (X_df[numeric_cols] - means) / stds\n",
    "\n",
    "# Convertir a numpy y normalizar L2\n",
    "X = X_df.values.astype(np.float64)\n",
    "X = normalize(X)\n",
    "\n",
    "# Features polinomiales\n",
    "X_poly_df = add_polynomial_features(X)\n",
    "\n",
    "# Normalizar features extendidas\n",
    "means_poly = X_poly_df.mean()\n",
    "stds_poly = X_poly_df.std()\n",
    "X_poly_df = (X_poly_df - means_poly) / stds_poly\n",
    "X_final_poly = X_poly_df.values\n",
    "\n",
    "# Entrenar el modelo final\n",
    "theta, bias = train_logistic_regression(X_final_poly, y, lr=0.1, epochs=1000)\n",
    "\n",
    "# Generar predicciones para los mismos datos de entrenamiento\n",
    "y_probs = sigmoid(np.dot(X_final_poly, theta) + bias)\n",
    "\n",
    "# Usar el mejor threshold encontrado\n",
    "best_thresh = 0.25\n",
    "y_preds = (y_probs >= best_thresh).astype(int).flatten()\n",
    "\n",
    "# Crear archivo de submission con train\n",
    "submission_df = pd.DataFrame({\n",
    "    \"paciente_id\": paciente_ids_train,\n",
    "    \"target\": y_preds\n",
    "})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"‚úÖ submission.csv generado con threshold = 0.25 usando train_df.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab413f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Funciones necesarias\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def normalize(X):\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1\n",
    "    return X / norms\n",
    "\n",
    "def train_logistic_regression(X, y, lr=0.2, epochs=2000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    bias = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        z = np.dot(X, theta) + bias\n",
    "        h = sigmoid(z)\n",
    "        error = h - y\n",
    "        grad_theta = np.dot(X.T, error) / m\n",
    "        grad_bias = np.mean(error)\n",
    "\n",
    "        theta -= lr * grad_theta\n",
    "        bias -= lr * grad_bias\n",
    "\n",
    "    return theta, bias\n",
    "\n",
    "def add_polynomial_and_cross_features_np(X):\n",
    "    \"\"\"\n",
    "    Agrega t√©rminos cuadr√°ticos, c√∫bicos y cruzados usando solo numpy.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    features = [X]\n",
    "\n",
    "    # Cuadrados y cubos\n",
    "    features.append(X ** 2)\n",
    "    features.append(X ** 3)\n",
    "    features.append(X ** 4)\n",
    "\n",
    "    # Interacciones cruzadas (combinaciones √∫nicas i < j)\n",
    "    cross_terms = []\n",
    "    for i in range(n_features):\n",
    "        for j in range(i + 1, n_features):\n",
    "            cross = (X[:, i] * X[:, j]).reshape(-1, 1)\n",
    "            cross_terms.append(cross)\n",
    "    if cross_terms:\n",
    "        features.append(np.hstack(cross_terms))\n",
    "\n",
    "    return np.hstack(features)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    return f1\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    thresholds = np.linspace(0.01, 0.9, 200)\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    for thresh in thresholds:\n",
    "        preds = (y_probs >= thresh).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = thresh\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "# === Cargar y procesar datos ===\n",
    "df = pd.read_csv(\"train_df.csv\")\n",
    "df[\"genero\"] = df[\"genero\"].map({\"M\": 1, \"F\": 0})\n",
    "y = df[\"target\"].values.reshape(-1, 1)\n",
    "paciente_ids = df[\"paciente_id\"]\n",
    "X_df = df.drop(columns=[\"paciente_id\", \"target\"])\n",
    "\n",
    "# Normalizaci√≥n num√©rica\n",
    "numeric_cols = X_df.columns.tolist()\n",
    "numeric_cols.remove(\"genero\")\n",
    "means = X_df[numeric_cols].mean()\n",
    "stds = X_df[numeric_cols].std()\n",
    "X_df[numeric_cols] = (X_df[numeric_cols] - means) / stds\n",
    "\n",
    "# L2 normalization\n",
    "X = normalize(X_df.values.astype(np.float64))\n",
    "\n",
    "# Features polinomiales y c√∫bicas cruzadas\n",
    "X_poly = add_polynomial_and_cross_features_np(X)\n",
    "\n",
    "# Normalizaci√≥n polinomial\n",
    "X_poly = (X_poly - X_poly.mean(axis=0)) / (X_poly.std(axis=0) + 1e-8)\n",
    "\n",
    "# Entrenar modelo\n",
    "theta, bias = train_logistic_regression(X_poly, y, lr=0.2, epochs=2000)\n",
    "\n",
    "# Predecir probabilidades\n",
    "y_probs = sigmoid(np.dot(X_poly, theta) + bias)\n",
    "\n",
    "# Buscar mejor threshold\n",
    "best_thresh, best_f1 = find_best_threshold(y.flatten(), y_probs.flatten())\n",
    "print(f\"üìä Mejor threshold encontrado: {best_thresh:.4f} con F1-score: {best_f1:.5f}\")\n",
    "\n",
    "# Predicci√≥n final\n",
    "y_preds = (y_probs >= best_thresh).astype(int).flatten()\n",
    "\n",
    "# Guardar submission\n",
    "submission_df = pd.DataFrame({\n",
    "    \"paciente_id\": paciente_ids,\n",
    "    \"target\": y_preds\n",
    "})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"‚úÖ submission.csv generado con features extendidas y threshold optimizado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Funciones base ===\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def normalize(X):\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1\n",
    "    return X / norms\n",
    "\n",
    "def standardize(X):\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0) + 1e-8\n",
    "    return (X - mean) / std, mean, std\n",
    "\n",
    "def train_logistic_regression(X, y, lr=0.1, epochs=10000, l2_lambda=0.01, early_stopping_rounds=300):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    bias = 0\n",
    "    best_loss = np.inf\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        z = np.dot(X, theta) + bias\n",
    "        h = sigmoid(z)\n",
    "        error = h - y\n",
    "\n",
    "        loss = -np.mean(y * np.log(h + 1e-8) + (1 - y) * np.log(1 - h + 1e-8)) + (l2_lambda / (2 * m)) * np.sum(theta**2)\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_theta, best_bias = theta.copy(), bias\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= early_stopping_rounds:\n",
    "                break\n",
    "\n",
    "        grad_theta = np.dot(X.T, error) / m + (l2_lambda / m) * theta\n",
    "        grad_bias = np.mean(error)\n",
    "\n",
    "        theta -= lr * grad_theta\n",
    "        bias -= lr * grad_bias\n",
    "\n",
    "    return best_theta, best_bias\n",
    "\n",
    "def add_polynomial_cross_features(X, max_degree=3):\n",
    "    n_samples, n_features = X.shape\n",
    "    features = [X]\n",
    "\n",
    "    for deg in range(2, max_degree + 1):\n",
    "        features.append(X ** deg)\n",
    "\n",
    "    cross_terms = []\n",
    "    for i in range(n_features):\n",
    "        for j in range(i + 1, n_features):\n",
    "            cross = (X[:, i] * X[:, j]).reshape(-1, 1)\n",
    "            cross_terms.append(cross)\n",
    "    if cross_terms:\n",
    "        features.append(np.hstack(cross_terms))\n",
    "\n",
    "    return np.hstack(features)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    thresholds = np.linspace(0.01, 0.99, 500)\n",
    "    best_thresh, best_f1 = 0.5, 0\n",
    "    for t in thresholds:\n",
    "        preds = (y_probs >= t).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thresh = f1, t\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "def stratified_kfold_indices(y, k=5, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    neg_idx = np.where(y == 0)[0]\n",
    "    np.random.shuffle(pos_idx)\n",
    "    np.random.shuffle(neg_idx)\n",
    "\n",
    "    pos_folds = np.array_split(pos_idx, k)\n",
    "    neg_folds = np.array_split(neg_idx, k)\n",
    "\n",
    "    folds = [np.concatenate((pos, neg)) for pos, neg in zip(pos_folds, neg_folds)]\n",
    "    return folds\n",
    "\n",
    "# === Preprocesamiento ===\n",
    "df = pd.read_csv(\"train_df.csv\")\n",
    "df[\"genero\"] = df[\"genero\"].map({\"M\": 1, \"F\": 0})\n",
    "y = df[\"target\"].values.reshape(-1, 1)\n",
    "X_df = df.drop(columns=[\"paciente_id\", \"target\"])\n",
    "\n",
    "X_df.fillna(X_df.median(), inplace=True)\n",
    "\n",
    "numeric_cols = X_df.columns.tolist()\n",
    "numeric_cols.remove(\"genero\")\n",
    "X_df[numeric_cols] = (X_df[numeric_cols] - X_df[numeric_cols].mean()) / (X_df[numeric_cols].std() + 1e-8)\n",
    "\n",
    "X = normalize(X_df.values.astype(np.float64))\n",
    "X_ext = add_polynomial_cross_features(X, max_degree=3)\n",
    "X_ext, _, _ = standardize(X_ext)\n",
    "\n",
    "# === Cross-validation ===\n",
    "folds = stratified_kfold_indices(y.flatten(), k=5)\n",
    "thresholds = []\n",
    "f1_scores = []\n",
    "\n",
    "for val_idx in folds:\n",
    "    train_idx = np.setdiff1d(np.arange(len(y)), val_idx)\n",
    "    X_train, y_train = X_ext[train_idx], y[train_idx]\n",
    "    X_val, y_val = X_ext[val_idx], y[val_idx]\n",
    "\n",
    "    theta, bias = train_logistic_regression(X_train, y_train, lr=0.1, epochs=10000, l2_lambda=0.01)\n",
    "    val_probs = sigmoid(np.dot(X_val, theta) + bias)\n",
    "    best_thresh, best_f1 = find_best_threshold(y_val.flatten(), val_probs.flatten())\n",
    "\n",
    "    thresholds.append(best_thresh)\n",
    "    f1_scores.append(best_f1)\n",
    "\n",
    "avg_thresh = np.mean(thresholds)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "print(f\"üìä Cross-validated threshold: {avg_thresh:.4f}, Avg F1-score: {avg_f1:.5f}\")\n",
    "\n",
    "# === Entrenamiento final ===\n",
    "theta, bias = train_logistic_regression(X_ext, y, lr=0.1, epochs=10000, l2_lambda=0.01)\n",
    "y_probs = sigmoid(np.dot(X_ext, theta) + bias)\n",
    "y_preds = (y_probs >= avg_thresh).astype(int).flatten()\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"paciente_id\": df[\"paciente_id\"],\n",
    "    \"target\": y_preds\n",
    "})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"‚úÖ submission.csv generado con validaci√≥n y entrenamiento final.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "def plot_f1_vs_threshold(y_true, y_probs):\n",
    "    thresholds = np.linspace(0.01, 0.99, 200)\n",
    "    f1_scores = []\n",
    "    for thresh in thresholds:\n",
    "        preds = (y_probs >= thresh).astype(int)\n",
    "        f1_scores.append(f1_score(y_true, preds))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(thresholds, f1_scores, label='F1-score')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1-score')\n",
    "    plt.title('F1-score vs Threshold')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso:\n",
    "plot_f1_vs_threshold(y.flatten(), y_probs.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa54267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Funciones\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    thresholds = np.linspace(0.01, 0.99, 500)\n",
    "    best_thresh, best_f1 = 0.5, 0\n",
    "    for t in thresholds:\n",
    "        preds = (y_probs >= t).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thresh = f1, t\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "def train_logistic_gradient_descent(X, y, lr=0.05, epochs=5000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    bias = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        z = np.dot(X, theta) + bias\n",
    "        h = sigmoid(z)\n",
    "        error = h - y\n",
    "\n",
    "        grad_theta = np.dot(X.T, error) / m\n",
    "        grad_bias = np.mean(error)\n",
    "\n",
    "        theta -= lr * grad_theta\n",
    "        bias -= lr * grad_bias\n",
    "\n",
    "    return theta, bias\n",
    "\n",
    "# === Cargar datos ===\n",
    "df = pd.read_csv(\"train_df.csv\")\n",
    "df[\"genero\"] = df[\"genero\"].map({\"M\": 1, \"F\": 0})\n",
    "y = df[\"target\"].values.reshape(-1, 1)\n",
    "X_df = df.drop(columns=[\"paciente_id\", \"target\"])\n",
    "\n",
    "# Imputar y estandarizar\n",
    "X_df.fillna(X_df.median(), inplace=True)\n",
    "num_cols = X_df.columns.drop(\"genero\")\n",
    "X_df[num_cols] = (X_df[num_cols] - X_df[num_cols].mean()) / (X_df[num_cols].std() + 1e-8)\n",
    "\n",
    "# Construcci√≥n de features cruzadas\n",
    "X = X_df.values.astype(np.float64)\n",
    "cross_terms = []\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(i + 1, X.shape[1]):\n",
    "        cross = (X[:, i] * X[:, j]).reshape(-1, 1)\n",
    "        cross_terms.append(cross)\n",
    "X_cross = np.hstack([X] + cross_terms)\n",
    "\n",
    "# Re-escalar despu√©s de cruzadas\n",
    "X_mean = X_cross.mean(axis=0)\n",
    "X_std = X_cross.std(axis=0) + 1e-8\n",
    "X_scaled = (X_cross - X_mean) / X_std\n",
    "\n",
    "# Entrenar modelo\n",
    "theta, bias = train_logistic_gradient_descent(X_scaled, y, lr=0.1, epochs=8000)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_probs = sigmoid(np.dot(X_scaled, theta) + bias)\n",
    "best_thresh, best_f1 = find_best_threshold(y.flatten(), y_probs.flatten())\n",
    "y_preds = (y_probs >= best_thresh).astype(int).flatten()\n",
    "\n",
    "print(f\"‚úÖ Mejor threshold: {best_thresh:.4f} con F1-score: {best_f1:.5f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "submission = pd.DataFrame({\n",
    "    \"paciente_id\": df[\"paciente_id\"],\n",
    "    \"target\": y_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"üìÅ Archivo submission.csv guardado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Funciones base ===\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    thresholds = np.linspace(0.01, 0.99, 500)\n",
    "    best_thresh, best_f1 = 0.5, 0\n",
    "    for t in thresholds:\n",
    "        preds = (y_probs >= t).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thresh = f1, t\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "def train_logistic_gradient_descent(X, y, lr=0.05, epochs=5000, lambda_=0.01):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    bias = 0\n",
    "    losses = []\n",
    "\n",
    "    pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        z = np.dot(X, theta) + bias\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        weight = np.where(y == 1, pos_weight, 1).reshape(-1, 1)\n",
    "        error = (h - y) * weight\n",
    "\n",
    "        grad_theta = (np.dot(X.T, error) / m) + (lambda_ / m) * theta\n",
    "        grad_bias = np.mean(error)\n",
    "\n",
    "        theta -= lr * grad_theta\n",
    "        bias -= lr * grad_bias\n",
    "\n",
    "        # P√©rdida (log loss)\n",
    "        loss = -np.mean(y * np.log(h + 1e-8) + (1 - y) * np.log(1 - h + 1e-8))\n",
    "        losses.append(loss)\n",
    "\n",
    "    return theta, bias, losses\n",
    "\n",
    "# === Cargar datos ===\n",
    "df = pd.read_csv(\"train_df.csv\")\n",
    "df[\"genero\"] = df[\"genero\"].map({\"M\": 1, \"F\": 0})\n",
    "y = df[\"target\"].values.reshape(-1, 1)\n",
    "X_df = df.drop(columns=[\"paciente_id\", \"target\"])\n",
    "\n",
    "# Imputar y estandarizar variables num√©ricas\n",
    "X_df.fillna(X_df.median(), inplace=True)\n",
    "num_cols = X_df.columns.drop(\"genero\")\n",
    "X_df[num_cols] = (X_df[num_cols] - X_df[num_cols].mean()) / (X_df[num_cols].std() + 1e-8)\n",
    "\n",
    "# === Ingenier√≠a de caracter√≠sticas ===\n",
    "X = X_df.values.astype(np.float64)\n",
    "features = [X]\n",
    "\n",
    "# Cuadrados, ra√≠ces y logaritmos\n",
    "for i in range(X.shape[1]):\n",
    "    col = X[:, i].reshape(-1, 1)\n",
    "    features.append(col ** 2)\n",
    "    features.append(np.sqrt(np.abs(col) + 1e-8))\n",
    "    features.append(np.log(np.abs(col) + 1e-8))\n",
    "\n",
    "# Interacciones cruzadas\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(i + 1, X.shape[1]):\n",
    "        cross = (X[:, i] * X[:, j]).reshape(-1, 1)\n",
    "        features.append(cross)\n",
    "\n",
    "X_ext = np.hstack(features)\n",
    "\n",
    "# Reescalar despu√©s de transformaci√≥n\n",
    "X_mean = X_ext.mean(axis=0)\n",
    "X_std = X_ext.std(axis=0) + 1e-8\n",
    "X_scaled = (X_ext - X_mean) / X_std\n",
    "\n",
    "# === Entrenamiento del modelo ===\n",
    "theta, bias, losses = train_logistic_gradient_descent(X_scaled, y, lr=0.1, epochs=8000, lambda_=0.01)\n",
    "\n",
    "# === Evaluaci√≥n ===\n",
    "y_probs = sigmoid(np.dot(X_scaled, theta) + bias)\n",
    "best_thresh, best_f1 = find_best_threshold(y.flatten(), y_probs.flatten())\n",
    "y_preds = (y_probs >= best_thresh).astype(int).flatten()\n",
    "\n",
    "print(f\"‚úÖ Mejor threshold: {best_thresh:.4f} con F1-score: {best_f1:.5f}\")\n",
    "\n",
    "# === Guardar resultados ===\n",
    "submission = pd.DataFrame({\n",
    "    \"paciente_id\": df[\"paciente_id\"],\n",
    "    \"target\": y_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"üìÅ Archivo submission.csv guardado.\")\n",
    "\n",
    "# === Graficar la p√©rdida ===\n",
    "plt.plot(losses)\n",
    "plt.title(\"Curva de p√©rdida\")\n",
    "plt.xlabel(\"√âpocas\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cbfd1e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Early stopping at epoch 94, loss: 0.685071\n",
      "‚úÖ Mejor threshold: 0.3851 con F1-score: 0.66978\n",
      "üìÅ Archivo submission.csv guardado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Funciones ===\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    thresholds = np.linspace(0.01, 0.99, 500)\n",
    "    best_thresh, best_f1 = 0.5, 0\n",
    "    for t in thresholds:\n",
    "        preds = (y_probs >= t).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thresh = f1, t\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "def remove_outliers(X, z_thresh=4):\n",
    "    z_scores = (X - X.mean()) / (X.std() + 1e-8)\n",
    "    mask = (np.abs(z_scores) < z_thresh).all(axis=1)\n",
    "    return X[mask], mask\n",
    "\n",
    "def train_logistic_gradient_descent(X, y, lr=0.1, max_epochs=8000, early_stopping_rounds=50):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    bias = 0\n",
    "    prev_loss = float('inf')\n",
    "    patience = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        z = np.dot(X, theta) + bias\n",
    "        h = sigmoid(z)\n",
    "        loss = -np.mean(y * np.log(h + 1e-8) + (1 - y) * np.log(1 - h + 1e-8))\n",
    "\n",
    "        if loss > prev_loss - 1e-5:\n",
    "            patience += 1\n",
    "        else:\n",
    "            patience = 0\n",
    "        if patience >= early_stopping_rounds:\n",
    "            print(f\"üõë Early stopping at epoch {epoch}, loss: {loss:.6f}\")\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "        error = h - y\n",
    "        grad_theta = np.dot(X.T, error) / m\n",
    "        grad_bias = np.mean(error)\n",
    "        theta -= lr * grad_theta\n",
    "        bias -= lr * grad_bias\n",
    "    return theta, bias\n",
    "\n",
    "# === Cargar y preparar datos ===\n",
    "df = pd.read_csv(\"train_df.csv\")\n",
    "df[\"genero\"] = df[\"genero\"].map({\"M\": 1, \"F\": 0})\n",
    "y = df[\"target\"].values.reshape(-1, 1)\n",
    "X_df = df.drop(columns=[\"paciente_id\", \"target\"])\n",
    "\n",
    "# Imputaci√≥n y normalizaci√≥n robusta\n",
    "X_df.fillna(X_df.median(), inplace=True)\n",
    "num_cols = X_df.columns.drop(\"genero\")\n",
    "X_df[num_cols] = (X_df[num_cols] - X_df[num_cols].median()) / (X_df[num_cols].std() + 1e-8)\n",
    "\n",
    "# Features cruzadas\n",
    "X = X_df.values.astype(np.float64)\n",
    "cross_terms = []\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(i + 1, X.shape[1]):\n",
    "        cross_terms.append((X[:, i] * X[:, j]).reshape(-1, 1))\n",
    "X_cross = np.hstack([X] + cross_terms)\n",
    "\n",
    "# Outlier removal\n",
    "X_filtered, mask = remove_outliers(pd.DataFrame(X_cross))\n",
    "y_filtered = y[mask]\n",
    "\n",
    "# Selecci√≥n de variables por correlaci√≥n con y\n",
    "X_array = X_filtered.to_numpy()\n",
    "y_array = y_filtered.flatten()\n",
    "\n",
    "\n",
    "corrs = np.array([np.corrcoef(X_array[:, i], y_array)[0, 1] for i in range(X_array.shape[1])])\n",
    "top_k = 60\n",
    "top_idx = np.argsort(np.abs(corrs))[-top_k:]\n",
    "X_selected = X_array[:, top_idx]\n",
    "\n",
    "\n",
    "# PCA (reducimos a 25 componentes)\n",
    "X_mean = X_selected.mean(axis=0)\n",
    "X_std = X_selected.std(axis=0) + 1e-8\n",
    "X_scaled = (X_selected - X_mean) / X_std\n",
    "\n",
    "cov_matrix = np.cov(X_scaled, rowvar=False)\n",
    "eig_vals, eig_vecs = np.linalg.eigh(cov_matrix)\n",
    "sorted_idx = np.argsort(eig_vals)[::-1][:25]\n",
    "top_components = eig_vecs[:, sorted_idx]\n",
    "X_pca = X_scaled @ top_components\n",
    "\n",
    "# Entrenar modelo con early stopping\n",
    "theta, bias = train_logistic_gradient_descent(X_pca, y_filtered, lr=0.15)\n",
    "\n",
    "# Predicci√≥n y threshold\n",
    "y_probs = sigmoid(np.dot(X_pca, theta) + bias)\n",
    "best_thresh, best_f1 = find_best_threshold(y_filtered.flatten(), y_probs.flatten())\n",
    "y_preds = (y_probs >= best_thresh).astype(int).flatten()\n",
    "\n",
    "print(f\"‚úÖ Mejor threshold: {best_thresh:.4f} con F1-score: {best_f1:.5f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "submission = pd.DataFrame({\n",
    "    \"paciente_id\": df.loc[mask, \"paciente_id\"],\n",
    "    \"target\": y_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"üìÅ Archivo submission.csv guardado.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
